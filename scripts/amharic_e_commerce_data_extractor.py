# -*- coding: utf-8 -*-
"""Amharic E-commerce Data Extractor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ULjlKf0PfF4XEo8hzVqrsrAFUKMTsjVA

***#Set Up a Data Ingestion Pipeline***
"""

#install telethon first
!pip install telethon
!pip install python-dotenv

"""***Fetch Data from Telegram Channels***"""

import nest_asyncio
nest_asyncio.apply()

import asyncio
from telethon import TelegramClient
from telethon.tl.functions.messages import GetHistoryRequest
import pandas as pd
from datetime import datetime

# Credentials
api_id = 25367099
api_hash = 'd7e39750bed65553fbceb37465d7689d'
phone = '+251966883822'

# Create client instance
client = TelegramClient('session_name', api_id, api_hash)

# Channel list
channels = [
    '@ZemenExpress',
    '@nevacomputer',
    '@ethio_brand_collection',
    '@meneshayeofficial',
    '@Leyueqa'
]

async def fetch_messages():
    await client.start(phone)

    all_data = []

    for ch in channels:
        try:
            entity = await client.get_entity(ch)

            messages = await client(GetHistoryRequest(
                peer=entity,
                limit=1000,  # Number of messages to fetch per channel
                offset_date=None,
                offset_id=0,
                max_id=0,
                min_id=0,
                add_offset=0,
                hash=0
            ))

            for msg in messages.messages:
                if hasattr(msg, 'message'):  # Check if message has text content
                    message_data = {
                        "channel": ch,
                        "date": msg.date,
                        "text": msg.message,
                        "views": getattr(msg, 'views', None),  # Some messages might not have views
                        "sender_id": getattr(msg, 'sender_id', None),
                        "message_id": msg.id,
                        "media_type": None
                    }

                    # Check for media
                    if hasattr(msg, 'media'):
                        if hasattr(msg.media, 'photo'):
                            message_data['media_type'] = 'photo'
                        elif hasattr(msg.media, 'document'):
                            message_data['media_type'] = 'document'

                    all_data.append(message_data)

        except Exception as e:
            print(f"Error in {ch}: {str(e)}")
            continue  # Continue with next channel if error occurs

    if all_data:
        df = pd.DataFrame(all_data)
        df.to_csv("telegram_scraped_data.csv", index=False)
        print(f"Successfully saved {len(df)} messages to telegram_scraped_data.csv")
        print(df.head())
    else:
        print("No messages were fetched. Check your channel list and connection.")

# Run the async task
await fetch_messages()

import pandas as pd

# Load and display the CSV as a DataFrame
df = pd.read_csv('telegram_scraped_data.csv')

# Display the first 5 rows (or change to df.head(10) for 10 rows, etc.)
df.head(1000)

"""Setup"""

!pip install etnltk

"""***Amharic text preprocessing with Amharic document***"""

from etnltk import Amharic
import pandas as pd

# Load the CSV
df = pd.read_csv('telegram_scraped_data.csv')

# Use the correct column name (e.g., 'text' instead of 'Text')
df['Cleaned_Text'] = df['text'].apply(lambda x: str(Amharic(str(x))) if pd.notnull(x) else '')

# Show cleaned examples
df[['text', 'Cleaned_Text']].head(1000)

"""***Clean and Structure the Data***"""

# 1. Install etnltk
!pip install -U etnltk

# 2. Imports
import pandas as pd
import re
from etnltk.lang.am import clean_amharic

# 3. Define custom cleaning functions
def remove_emojis(text):
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

def remove_digits(text):
    return re.sub(r'\d+', '', text)

def remove_english_chars(text):
    return re.sub(r'[A-Za-z]', '', text)

def remove_ethiopic_punctuation(text):
    ethiopic_punct = "፡።፣፤፥፦፧፨"
    return re.sub(f"[{ethiopic_punct}]", '', text)

# 4. Load the CSV
df = pd.read_csv('telegram_scraped_data.csv')

# 5. Normalize column names
df.columns = df.columns.str.strip().str.lower()

# 6. Confirm the text column exists
if 'text' not in df.columns:
    raise KeyError("Expected a column named 'text' in the CSV.")

# 7. Define cleaning pipeline
custom_pipeline = [
    remove_emojis,
    remove_digits,
    remove_english_chars,
    remove_ethiopic_punctuation
]

# 8. Apply cleaning
df['cleaned_text'] = df['text'].apply(
    lambda x: clean_amharic(str(x), pipeline=custom_pipeline) if pd.notnull(x) else ''
)

# 9. Preview
df[['text', 'cleaned_text']].head(100)

"""Tokenization"""

from etnltk import Amharic

# Helper function to process non-empty cleaned text
def extract_sentences(text):
    if pd.notnull(text) and text.strip() != '':
        return Amharic(text).sentences
    return []

# Apply to each row safely
df['sentences'] = df['cleaned_text'].apply(extract_sentences)

# Preview
df[['cleaned_text', 'sentences']].head(1000)

"""Tokenization"""

from etnltk.tokenize.am import word_tokenize

# Tokenize each cleaned_text entry (if not empty)
df['tokens'] = df['cleaned_text'].apply(
    lambda x: word_tokenize(x) if pd.notnull(x) and x.strip() != '' else []
)

# Preview some tokens
df[['cleaned_text', 'tokens']].head(1000)

df['tokens'] = df['text'].apply(
    lambda x: word_tokenize(x) if pd.notnull(x) and x.strip() != '' else []
)
print(df[['text', 'tokens']].head(1000))

# Example: Clean raw text (remove unwanted characters, normalize, etc.)
df['cleaned_text'] = df['text'].str.replace(r'[^\u1200-\u137F\s]', '', regex=True)  # Keep Ge'ez script and spaces

import pandas as pd
from etnltk.tokenize.am import word_tokenize

# Create subset (e.g., first 100 rows)
subset_df = df.head(100)  # Adjust as needed

# Save tokens to a text file for manual annotation
with open('amharic_tokens_for_labeling.txt', 'w', encoding='utf-8') as f:
    for idx, row in subset_df.iterrows():
        if row['tokens']:  # Skip empty token lists
            f.write(f"# text: {row['text']}\n")  # Use correct column name
            for token in row['tokens']:
                f.write(f"{token}\t_\t_\n")  # Placeholder for POS, NER
            f.write("\n")

# Add placeholder tags (replace with actual annotations if available)
subset_df['ner_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['O'] * len(tokens) if tokens else []
)
subset_df['pos_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['_'] * len(tokens) if tokens else []
)

# Save to CoNLL
def save_to_conll(df, output_file, token_col='tokens', ner_col='ner_tags', pos_col='pos_tags'):
    with open(output_file, 'w', encoding='utf-8') as f:
        for idx, row in df.iterrows():
            if not row[token_col]:  # Skip empty token lists
                continue
            f.write(f"# text: {row['text']}\n")  # Use correct column name
            tokens = row[token_col]
            ner_tags = row[ner_col] if ner_col in df.columns else ['O'] * len(tokens)
            pos_tags = row[pos_col] if pos_col in df.columns else ['_'] * len(tokens)

            for token, pos, ner in zip(tokens, pos_tags, ner_tags):
                f.write(f"{token}\t{pos}\t{ner}\n")
            f.write("\n")

save_to_conll(subset_df, 'amharic_ner.conll')

import pandas as pd
from etnltk.tokenize.am import word_tokenize

# Assuming df is your DataFrame
subset_df = df.head(100)
subset_df['ner_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['O'] * len(tokens) if tokens else []
)
subset_df['pos_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['_'] * len(tokens) if tokens else []
)

def save_to_conll(df, output_file, token_col='tokens', ner_col='ner_tags', pos_col='pos_tags'):
    with open(output_file, 'w', encoding='utf-8') as f:
        for idx, row in df.iterrows():
            if not row[token_col]:  # Skip empty token lists
                continue
            f.write(f"# text: {row['text']}\n")  # Assuming 'text' column
            tokens = row[token_col]
            ner_tags = row[ner_col] if ner_col in df.columns else ['O'] * len(tokens)
            pos_tags = row[pos_col] if pos_col in df.columns else ['_'] * len(tokens)

            for token, pos, ner in zip(tokens, pos_tags, ner_tags):
                f.write(f"{token}\t{pos}\t{ner}\n")
            f.write("\n")

save_to_conll(subset_df, 'amharic_ner.conll')

print(subset_df.columns)
print(subset_df[['tokens', 'ner_tags']].head(1000))

# Check for length mismatches
for idx, row in subset_df.iterrows():
    if row['tokens'] and len(row['tokens']) != len(row['ner_tags']):
        print(f"Row {idx}: Token-NER mismatch. Tokens: {len(row['tokens'])}, NER tags: {len(row['ner_tags'])}")
    if row['tokens'] and len(row['tokens']) != len(row['pos_tags']):
        print(f"Row {idx}: Token-POS mismatch. Tokens: {len(row['tokens'])}, POS tags: {len(row['pos_tags'])}")

subset_df['ner_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['O'] * len(tokens) if tokens else []
)
subset_df['pos_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['_'] * len(tokens) if tokens else []
)

save_to_conll(subset_df, 'amharic_ner.conll')

with open('amharic_ner.conll', 'r', encoding='utf-8') as f:
    print("\n".join(f.readlines()[:1000]))

import pandas as pd
from etnltk.tokenize.am import word_tokenize
import re

# Verify DataFrame exists
if 'df' not in globals():
    raise NameError("DataFrame 'df' is not defined. Load or create it first.")
if 'text' not in df.columns:
    raise KeyError("Column 'text' not found. Available columns: " + str(df.columns.tolist()))

# Create a copy of subset to avoid SettingWithCopyWarning
subset_df = df.head(100).copy()

# Tokenize if not already done
if 'tokens' not in subset_df.columns:
    subset_df['tokens'] = subset_df['text'].apply(
        lambda x: word_tokenize(x) if pd.notnull(x) and x.strip() != '' else []
    )

# Filter out rows with non-Amharic or problematic text
def is_valid_amharic_text(text):
    if pd.isna(text) or not text.strip():
        return False
    # Keep text with at least some Amharic characters (Ge'ez script: U+1200–U+137F)
    return bool(re.search(r'[\u1200-\u137F]', text))

subset_df = subset_df[subset_df['text'].apply(is_valid_amharic_text)]

# Add placeholder tags
subset_df.loc[:, 'ner_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['O'] * len(tokens) if tokens else []
)
subset_df.loc[:, 'pos_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['_'] * len(tokens) if tokens else []
)

# Debug: Check for empty tokens
empty_token_rows = subset_df[subset_df['tokens'].apply(len) == 0]
if not empty_token_rows.empty:
    print(f"Warning: {len(empty_token_rows)} rows have empty tokens:")
    print(empty_token_rows[['text', 'tokens']].head())

# Save to CoNLL
def save_to_conll(df, output_file, token_col='tokens', ner_col='ner_tags', pos_col='pos_tags'):
    with open(output_file, 'w', encoding='utf-8') as f:
        sentence_count = 0
        for idx, row in df.iterrows():
            if not row[token_col]:
                continue
            # Only include text with Amharic content in comments
            if not is_valid_amharic_text(row['text']):
                continue
            f.write(f"# text: {row['text']}\n")
            tokens = row[token_col]
            ner_tags = row[ner_col] if ner_col in df.columns else ['O'] * len(tokens)
            pos_tags = row[pos_col] if pos_col in df.columns else ['_'] * len(tokens)

            if len(tokens) != len(ner_tags) or len(tokens) != len(pos_tags):
                print(f"Row {idx}: Mismatch. Tokens: {len(tokens)}, NER: {len(ner_tags)}, POS: {len(pos_tags)}")
                continue

            for token, pos, ner in zip(tokens, pos_tags, ner_tags):
                # Skip tokens that are purely emojis or non-text
                if not re.match(r'^[\u1200-\u137F0-9a-zA-Z]+$', token):
                    continue
                f.write(f"{token}\t{pos}\t{ner}\n")
            f.write("\n")
            sentence_count += 1
        print(f"Wrote {sentence_count} sentences to {output_file}")

save_to_conll(subset_df, 'amharic_ner.conll')

# Validate
from conllu import parse
try:
    with open('amharic_ner.conll', 'r', encoding='utf-8') as f:
        data = f.read()
    sentences = parse(data)
    print(f"Parsed {len(sentences)} sentences")
except Exception as e:
    print(f"Parsing failed: {e}")
    with open('amharic_ner.conll', 'r', encoding='utf-8') as f:
        print("\nFirst 20 lines of file:")
        print("\n".join(f.readlines()[:20]))

def rule_based_ner(tokens):
    ner_tags = []
    for i, token in enumerate(tokens):
        if token in {'መገናኛ', 'መሰረት', 'ደፋር', 'ሞል'}:
            ner_tags.append('B-LOC' if i == 0 or tokens[i-1] not in {'መገናኛ', 'መሰረት', 'ደፋር', 'ሞል'} else 'I-LOC')
        elif token.startswith('@'):
            ner_tags.append('B-ORG')
        elif token == 'ብር':
            ner_tags.append('I-PRICE' if i > 0 and tokens[i-1].isdigit() else 'O')
        elif token.isdigit():
            ner_tags.append('B-PRICE')
        else:
            ner_tags.append('O')
    return ner_tags

subset_df.loc[:, 'ner_tags'] = subset_df['tokens'].apply(rule_based_ner)
save_to_conll(subset_df, 'amharic_ner.conll')

import pandas as pd
from etnltk.tokenize.am import word_tokenize
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import re
from conllu import parse

# Verify DataFrame exists
if 'df' not in globals():
    raise NameError("DataFrame 'df' is not defined. Load or create it first.")
if 'text' not in df.columns:
    raise KeyError("Column 'text' not found. Available columns: " + str(df.columns.tolist()))

# Create a copy of subset to avoid SettingWithCopyWarning
subset_df = df.head(100).copy()

# Tokenize if not already done
if 'tokens' not in subset_df.columns:
    subset_df['tokens'] = subset_df['text'].apply(
        lambda x: word_tokenize(x) if pd.notnull(x) and x.strip() != '' else []
    )

# Filter out rows with non-Amharic or problematic text
def is_valid_amharic_text(text):
    if pd.isna(text) or not text.strip():
        return False
    return bool(re.search(r'[\u1200-\u137F]', text))  # Ge'ez script

subset_df = subset_df[subset_df['text'].apply(is_valid_amharic_text)]

# Debug: Check for empty tokens
empty_token_rows = subset_df[subset_df['tokens'].apply(len) == 0]
if not empty_token_rows.empty:
    print(f"Warning: {len(empty_token_rows)} rows have empty tokens:")
    print(empty_token_rows[['text', 'tokens']].head())

# Initialize NER pipeline
try:
    tokenizer = AutoTokenizer.from_pretrained("Davlan/bert-base-multilingual-cased-ner-hrl")
    model = AutoModelForTokenClassification.from_pretrained("Davlan/bert-base-multilingual-cased-ner-hrl")
    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")
except OSError as e:
    print(f"Error loading model: {e}")
    print("Ensure you have internet access and the correct model identifier.")
    raise

# Predict NER tags with token alignment
def predict_ner(tokens):
    if not tokens:
        return []
    text = " ".join(tokens)
    try:
        predictions = ner_pipeline(text)
    except Exception as e:
        print(f"NER pipeline failed for text: {text}\nError: {e}")
        return ['O'] * len(tokens)

    # Align predictions with tokens
    ner_tags = ['O'] * len(tokens)
    token_idx = 0
    current_offset = 0
    for pred in predictions:
        entity = pred['entity_group']
        start = pred['start']
        end = pred['end']
        # Find which token(s) the entity spans
        token_text = "".join(tokens)  # Concatenate without spaces for offset calculation
        while token_idx < len(tokens):
            token = tokens[token_idx]
            token_start = current_offset
            token_end = current_offset + len(token)
            if start >= token_end:
                current_offset += len(token) + 1  # +1 for space
                token_idx += 1
                continue
            if end <= token_start:
                break
            # Assign entity to the token (simplified: assign to first token of entity)
            ner_tags[token_idx] = entity
            break
    return ner_tags

# Apply NER predictions
subset_df.loc[:, 'ner_tags'] = subset_df['tokens'].apply(predict_ner)
subset_df.loc[:, 'pos_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['_'] * len(tokens) if tokens else []
)

# Save to CoNLL
def save_to_conll(df, output_file, token_col='tokens', ner_col='ner_tags', pos_col='pos_tags'):
    with open(output_file, 'w', encoding='utf-8') as f:
        sentence_count = 0
        for idx, row in df.iterrows():
            if not row[token_col]:
                continue
            if not is_valid_amharic_text(row['text']):
                continue
            f.write(f"# text: {row['text']}\n")
            tokens = row[token_col]
            ner_tags = row[ner_col] if ner_col in df.columns else ['O'] * len(tokens)
            pos_tags = row[pos_col] if pos_col in df.columns else ['_'] * len(tokens)

            if len(tokens) != len(ner_tags) or len(tokens) != len(pos_tags):
                print(f"Row {idx}: Mismatch. Tokens: {len(tokens)}, NER: {len(ner_tags)}, POS: {len(pos_tags)}")
                continue

            for token, pos, ner in zip(tokens, pos_tags, ner_tags):
                if not re.match(r'^[\u1200-\u137F0-9a-zA-Z]+$', token):
                    continue
                f.write(f"{token}\t{pos}\t{ner}\n")
            f.write("\n")
            sentence_count += 1
        print(f"Wrote {sentence_count} sentences to {output_file}")

save_to_conll(subset_df, 'amharic_ner.conll')

# Validate
try:
    with open('amharic_ner.conll', 'r', encoding='utf-8') as f:
        data = f.read()
    sentences = parse(data)
    print(f"Parsed {len(sentences)} sentences")
except Exception as e:
    print(f"Parsing failed: {e}")
    with open('amharic_ner.conll', 'r', encoding='utf-8') as f:
        print("\nFirst 20 lines of file:")
        print("\n".join(f.readlines()[:100]))

def rule_based_ner(tokens):
    ner_tags = []
    for i, token in enumerate(tokens):
        if token in {'መገናኛ', 'መሰረት', 'ደፋር', 'ሞል'}:
            ner_tags.append('B-LOC' if i == 0 or tokens[i-1] not in {'መገናኛ', 'መሰረት', 'ደፋር', 'ሞል'} else 'I-LOC')
        elif token.startswith('@'):
            ner_tags.append('B-ORG')
        elif token == 'ብር':
            ner_tags.append('I-PRICE' if i > 0 and tokens[i-1].isdigit() else 'O')
        elif token.isdigit():
            ner_tags.append('B-PRICE')
        else:
            ner_tags.append('O')
    return ner_tags

subset_df.loc[:, 'ner_tags'] = subset_df['tokens'].apply(rule_based_ner)
save_to_conll(subset_df, 'amharic_ner.conll')

with open('amharic_ner.conll', 'r', encoding='utf-8') as f:
    print("\n".join(f.readlines()[:20]))